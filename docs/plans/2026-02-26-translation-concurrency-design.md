# 全文翻译并发优化设计

**日期**: 2026-02-26
**状态**: 已确认，待实现

## 问题

全文翻译等待时间过长。根本原因是批次之间完全串行：`service.ts` 中的 `for` 循环每次 `await translateBatch()`，上一批未完成不会开始下一批。对于 50 段的文章需等待 5 次 LLM 调用依次完成，总耗时约 25-40 秒。

## 方案选择

选定 **方案 A：批次并发 + 调小 BATCH_SIZE**，理由：
- 改动集中在 `service.ts` 一处
- 不需要区分引擎类型，对 LLM/Google/Microsoft 均有效
- 并发 3 批理论上可将速度提升约 3 倍

## 设计细节

### 参数调整（`service.ts`）

| 参数 | 旧值 | 新值 | 原因 |
|------|------|------|------|
| `BATCH_SIZE` | 10 | 5 | 单批段落减少，单批响应更快 |
| `CONCURRENCY`（新增） | — | 3 | 同时进行的最大批次数 |

### 并发逻辑（`service.ts`）

将 `for` 串行循环替换为 sliding window 并发模式（无需第三方库）：

```
旧: 批次1 → await → 批次2 → await → 批次3 ...
新: 批次1、2、3 同时发 → 任一完成立刻发批次4 → ...
```

实现方式：将全部批次切好后，用并发槽池控制最多 3 个并发，任一批次完成后写库、广播进度、释放槽位。

**顺序保证**：每批段落 index 固定，写库时按 index 覆盖对应段落，并发不影响结果顺序。

**进度广播**：不变，每批完成后仍逐段广播 `translationOnProgress` 事件。

### LLM 引擎小调整（`llm-engine.ts`）

`maxOutputTokens` 估算系数从 `× 2` 降为 `× 1.5`：
- 中文翻译通常比英文原文更短
- `× 2` 过于保守，部分模型对高 max_tokens 响应更慢

## 预期效果

- 50 段文章：从约 25-40 秒 → 约 8-15 秒（3x 并发加速）
- 进度更新节奏不变，用户感知流畅度一致
- 失败重试逻辑不变，稳定性不降低
